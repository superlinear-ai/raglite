"""DuckDB or PostgreSQL database tables for RAGLite."""

import contextlib
import datetime
import json
from dataclasses import dataclass, field
from functools import lru_cache
from hashlib import sha256
from pathlib import Path
from typing import Any
from xml.sax.saxutils import escape

import numpy as np
from markdown_it import MarkdownIt
from packaging import version
from pydantic import ConfigDict
from sqlalchemy.engine import Engine, make_url
from sqlalchemy.exc import ProgrammingError
from sqlmodel import (
    JSON,
    Column,
    Field,
    Integer,
    Relationship,
    Sequence,
    Session,
    SQLModel,
    create_engine,
    text,
)

from raglite._config import RAGLiteConfig
from raglite._litellm import get_embedding_dim
from raglite._typing import (
    ChunkId,
    DocumentId,
    Embedding,
    EvalId,
    FloatMatrix,
    FloatVector,
    IndexId,
    PickledObject,
)


def hash_bytes(data: bytes, max_len: int = 16) -> str:
    """Hash bytes to a hexadecimal string."""
    return sha256(data, usedforsecurity=False).hexdigest()[:max_len]


class Document(SQLModel, table=True):
    """A document."""

    # Enable JSON columns.
    model_config = ConfigDict(arbitrary_types_allowed=True)  # type: ignore[assignment]

    # Table columns.
    id: DocumentId = Field(..., primary_key=True)
    filename: str
    url: str | None = Field(default=None)
    metadata_: dict[str, Any] = Field(default_factory=dict, sa_column=Column("metadata", JSON))

    # Add relationships so we can access document.chunks and document.evals.
    chunks: list["Chunk"] = Relationship(back_populates="document", cascade_delete=True)
    evals: list["Eval"] = Relationship(back_populates="document", cascade_delete=True)

    @staticmethod
    def from_path(doc_path: Path, **kwargs: Any) -> "Document":
        """Create a document from a file path."""
        return Document(
            id=hash_bytes(doc_path.read_bytes()),
            filename=doc_path.name,
            metadata_={
                "size": doc_path.stat().st_size,
                "created": doc_path.stat().st_ctime,
                "modified": doc_path.stat().st_mtime,
                **kwargs,
            },
        )

    @staticmethod
    def from_markdown(content: str, filename: str | None, **kwargs: Any) -> "Document":
        """Create a document from Markdown content.

        Parameters
        ----------
        content
            The document's content as a Markdown string.
        filename
            The document filename to use. If not provided, the first line of the content is used.
        kwargs
            Any additional metadata to store.

        Returns
        -------
        Document
            A document.
        """
        return Document(
            id=hash_bytes(content.encode()),
            filename=filename or (content.strip().split("\n", 1)[0].strip() + ".md"),
            metadata_={"size": len(content.encode()), **kwargs},
        )


class Chunk(SQLModel, table=True):
    """A document chunk."""

    # Enable JSON columns.
    model_config = ConfigDict(arbitrary_types_allowed=True)  # type: ignore[assignment]

    # Table columns.
    id: ChunkId = Field(..., primary_key=True)
    document_id: DocumentId = Field(..., foreign_key="document.id", index=True)
    index: int = Field(..., index=True)
    headings: str
    body: str
    metadata_: dict[str, Any] = Field(default_factory=dict, sa_column=Column("metadata", JSON))

    # Add relationships so we can access chunk.document and chunk.embeddings.
    document: Document = Relationship(back_populates="chunks")
    embeddings: list["ChunkEmbedding"] = Relationship(back_populates="chunk", cascade_delete=True)

    @staticmethod
    def from_body(
        document: Document, index: int, body: str, headings: str = "", **kwargs: Any
    ) -> "Chunk":
        """Create a chunk from Markdown."""
        return Chunk(
            id=hash_bytes(f"{document.id}-{index}".encode()),
            document_id=document.id,
            index=index,
            headings=Chunk.truncate_headings(headings, body),
            body=body,
            metadata_={"filename": document.filename, "url": document.url, **kwargs},
        )

    @staticmethod
    def extract_heading_lines(doc: str, leading_only: bool = False) -> list[str]:  # noqa: FBT001,FBT002
        """Extract the leading or final state of the Markdown headings of a document."""
        md = MarkdownIt()
        heading_lines = [""] * 6
        level = None
        for token in md.parse(doc):
            if token.type == "heading_open":
                level = int(token.tag[1]) if 1 <= int(token.tag[1]) <= 6 else None  # noqa: PLR2004
            elif token.type == "heading_close":
                level = None
            elif level is not None:
                heading_content = token.content.strip().replace("\n", " ")
                heading_lines[level - 1] = ("#" * level) + " " + heading_content
                heading_lines[level:] = [""] * len(heading_lines[level:])
            elif leading_only and level is None and token.content and not token.content.isspace():
                break
        return heading_lines

    @staticmethod
    def truncate_headings(headings: str, body: str) -> str:
        """Truncate the contextual headings given the chunk's leading headings (if present)."""
        heading_lines = Chunk.extract_heading_lines(headings)
        leading_body_heading_lines = Chunk.extract_heading_lines(body, leading_only=True)
        level = next((i + 1 for i, line in enumerate(leading_body_heading_lines) if line), None)
        if level:
            heading_lines[level - 1 :] = [""] * len(heading_lines[level - 1 :])
        headings = "\n".join([heading for heading in heading_lines if heading])
        return headings

    def extract_headings(self) -> str:
        """Extract Markdown headings from the chunk, starting from the contextual headings."""
        heading_lines = self.extract_heading_lines(self.headings + "\n\n" + self.body)
        headings = "\n".join([heading for heading in heading_lines if heading])
        return headings

    @property
    def embedding_matrix(self) -> FloatMatrix:
        """Return this chunk's multi-vector embedding matrix."""
        # Uses the relationship chunk.embeddings to access the chunk_embedding table.
        return np.vstack([embedding.embedding[np.newaxis, :] for embedding in self.embeddings])

    def __hash__(self) -> int:
        return hash(self.id)

    def __repr__(self) -> str:
        return json.dumps(
            {
                "id": self.id,
                "document_id": self.document_id,
                "index": self.index,
                "headings": self.headings,
                "body": self.body[:100],
                "metadata": self.metadata_,
            },
            indent=4,
        )

    @property
    def front_matter(self) -> str:
        """Return this chunk's front matter."""
        # Compose the chunk metadata from the filename and URL.
        metadata = "\n".join(
            f"{key}: {self.metadata_.get(key)}"
            for key in ("filename", "url")
            if self.metadata_.get(key)
        )
        if not metadata:
            return ""
        # Return the chunk metadata in the YAML front matter format [1].
        # [1] https://jekyllrb.com/docs/front-matter/
        front_matter = f"---\n{metadata}\n---"
        return front_matter

    @property
    def content(self) -> str:
        """Return this chunk's front matter, contextual heading, and body."""
        return f"{self.front_matter}\n\n{self.headings.strip()}\n\n{self.body.strip()}".strip()

    def __str__(self) -> str:
        """Return this chunk's content."""
        return self.content


@dataclass
class ChunkSpan:
    """A consecutive sequence of chunks from a single document."""

    chunks: list[Chunk]
    document: Document = field(init=False)

    def __post_init__(self) -> None:
        """Set the document field."""
        if self.chunks:
            self.document = self.chunks[0].document

    def to_xml(self, index: int | None = None) -> str:
        """Convert this chunk span to an XML representation.

        The XML representation follows Anthropic's best practices [1].

        [1] https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips
        """
        if not self.chunks:
            return ""
        index_attribute = f' index="{index}"' if index is not None else ""
        xml_document = "\n".join(
            [
                f'<document{index_attribute} id="{self.document.id}">',
                f"<source>{self.document.url if self.document.url else self.document.filename}</source>",
                f'<span from_chunk_id="{self.chunks[0].id}" to_chunk_id="{self.chunks[-1].id}">',
                f"<headings>\n{escape(self.chunks[0].headings.strip())}\n</headings>",
                f"<content>\n{escape(''.join(chunk.body for chunk in self.chunks).strip())}\n</content>",
                "</span>",
                "</document>",
            ]
        )
        return xml_document

    def to_json(self, index: int | None = None) -> str:
        """Convert this chunk span to a JSON representation.

        The JSON representation follows Anthropic's best practices [1].

        [1] https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips
        """
        if not self.chunks:
            return "{}"
        index_attribute = {"index": index} if index is not None else {}
        json_document = {
            **index_attribute,
            "id": self.document.id,
            "source": self.document.url if self.document.url else self.document.filename,
            "span": {
                "from_chunk_id": self.chunks[0].id,
                "to_chunk_id": self.chunks[-1].id,
                "headings": self.chunks[0].headings.strip(),
                "content": "".join(chunk.body for chunk in self.chunks).strip(),
            },
        }
        return json.dumps(json_document)

    @property
    def content(self) -> str:
        """Return this chunk span's front matter, contextual heading, and chunk bodies."""
        front_matter = self.chunks[0].front_matter if self.chunks else ""
        heading = self.chunks[0].headings.strip() if self.chunks else ""
        bodies = "".join(chunk.body for chunk in self.chunks)
        return f"{front_matter}\n\n{heading}\n\n{bodies}".strip()

    def __str__(self) -> str:
        """Return this chunk span's content."""
        return self.content


# We manually create an auto-incrementing sequence for `chunk_embedding.id` here because DuckDB
# doesn't support `id SERIAL` out of the box.
chunk_embedding_id_seq = Sequence("chunk_embedding_id_seq")


class ChunkEmbedding(SQLModel, table=True):
    """A (sub-)chunk embedding."""

    __tablename__ = "chunk_embedding"

    # Enable Embedding columns.
    model_config = ConfigDict(arbitrary_types_allowed=True)  # type: ignore[assignment]

    # Table columns.
    id: int | None = Field(
        default=None,
        sa_column=Column(
            Integer,
            chunk_embedding_id_seq,
            primary_key=True,
            server_default=chunk_embedding_id_seq.next_value(),
        ),
    )
    chunk_id: ChunkId = Field(..., foreign_key="chunk.id", index=True)
    embedding: FloatVector = Field(..., sa_column=Column(Embedding(dim=-1)))

    # Add relationship so we can access embedding.chunk.
    chunk: Chunk = Relationship(back_populates="embeddings")

    @classmethod
    def set_embedding_dim(cls, dim: int) -> None:
        """Modify the embedding column's dimension after class definition."""
        cls.__table__.c["embedding"].type.dim = dim  # type: ignore[attr-defined]


class IndexMetadata(SQLModel, table=True):
    """Vector and keyword search index metadata."""

    __tablename__ = "index_metadata"

    # Enable PickledObject columns.
    model_config = ConfigDict(arbitrary_types_allowed=True)  # type: ignore[assignment]

    # Table columns.
    id: IndexId = Field(..., primary_key=True)
    version: datetime.datetime = Field(
        default_factory=lambda: datetime.datetime.now(datetime.timezone.utc)
    )
    metadata_: dict[str, Any] = Field(
        default_factory=dict, sa_column=Column("metadata", PickledObject)
    )

    @staticmethod
    @lru_cache(maxsize=4)
    def _get(id_: str, *, config: RAGLiteConfig | None = None) -> dict[str, Any] | None:
        engine = create_database_engine(config)
        with Session(engine) as session:
            index_metadata_record = session.get(IndexMetadata, id_)
            if index_metadata_record is None:
                return None
        return index_metadata_record.metadata_

    @staticmethod
    def get(id_: str = "default", *, config: RAGLiteConfig | None = None) -> dict[str, Any]:
        metadata = IndexMetadata._get(id_, config=config) or {}
        return metadata


class Eval(SQLModel, table=True):
    """A RAG evaluation example."""

    __tablename__ = "eval"

    # Enable JSON columns.
    model_config = ConfigDict(arbitrary_types_allowed=True)  # type: ignore[assignment]

    # Table columns.
    id: EvalId = Field(..., primary_key=True)
    document_id: DocumentId = Field(..., foreign_key="document.id", index=True)
    chunk_ids: list[ChunkId] = Field(default_factory=list, sa_column=Column(JSON))
    question: str
    contexts: list[str] = Field(default_factory=list, sa_column=Column(JSON))
    ground_truth: str
    metadata_: dict[str, Any] = Field(default_factory=dict, sa_column=Column("metadata", JSON))

    # Add relationship so we can access eval.document.
    document: Document = Relationship(back_populates="evals")

    @staticmethod
    def from_chunks(
        question: str, contexts: list[Chunk], ground_truth: str, **kwargs: Any
    ) -> "Eval":
        """Create a chunk from Markdown."""
        document_id = contexts[0].document_id
        chunk_ids = [context.id for context in contexts]
        return Eval(
            id=hash_bytes(f"{document_id}-{chunk_ids}-{question}".encode()),
            document_id=document_id,
            chunk_ids=chunk_ids,
            question=question,
            contexts=[str(context) for context in contexts],
            ground_truth=ground_truth,
            metadata_=kwargs,
        )


@lru_cache(maxsize=1)
def create_database_engine(config: RAGLiteConfig | None = None) -> Engine:  # noqa: C901, PLR0912, PLR0915
    """Create a database engine and initialize it."""
    # Parse the database URL and validate that the database backend is supported.
    config = config or RAGLiteConfig()
    db_url = make_url(config.db_url)
    db_backend = db_url.get_backend_name()
    # Update database configuration.
    connect_args = {}
    if db_backend == "postgresql":
        # Select the pg8000 driver if not set (psycopg2 is the default), and prefer SSL.
        if "+" not in db_url.drivername:
            db_url = db_url.set(drivername="postgresql+pg8000")
        # Support setting the sslmode for pg8000.
        if "pg8000" in db_url.drivername and "sslmode" in db_url.query:
            query = dict(db_url.query)
            if query.pop("sslmode") != "disable":
                connect_args["ssl_context"] = True
            db_url = db_url.set(query=query)
    elif db_backend == "duckdb":
        with contextlib.suppress(Exception):
            if db_url.database and db_url.database != ":memory:":
                Path(db_url.database).parent.mkdir(parents=True, exist_ok=True)
    else:
        error_message = "RAGLite only supports DuckDB or PostgreSQL."
        raise ValueError(error_message)
    # Create the engine.
    engine = create_engine(db_url, pool_pre_ping=True, connect_args=connect_args)
    # Install database extensions.
    if db_backend == "postgresql":
        with Session(engine) as session:
            session.execute(text("CREATE EXTENSION IF NOT EXISTS vector;"))
            session.commit()
    elif db_backend == "duckdb":
        with Session(engine) as session:
            session.execute(text("INSTALL fts; LOAD fts;"))
            session.execute(text("INSTALL vss; LOAD vss;"))
            session.commit()
    # Get the embedding dimension.
    embedding_dim = get_embedding_dim(config)
    # Create all SQLModel tables.
    ChunkEmbedding.set_embedding_dim(embedding_dim)
    SQLModel.metadata.create_all(engine)
    # Create backend-specific indexes.
    ef_search = (10 * 4) * 4  # This is (number of results with reranking) * oversampling factor.
    if db_backend == "postgresql":
        # Create a keyword search index with `tsvector` and a vector search index with `pgvector`.
        with Session(engine) as session:
            session.execute(
                text("""
                CREATE INDEX IF NOT EXISTS keyword_search_chunk_index ON chunk USING GIN (to_tsvector('simple', body));
                """)
            )
            metrics = {"cosine": "cosine", "dot": "ip", "l1": "l1", "l2": "l2"}
            create_vector_index_sql = f"""
                CREATE INDEX IF NOT EXISTS vector_search_chunk_index ON chunk_embedding
                USING hnsw (
                    (embedding::halfvec({embedding_dim}))
                    halfvec_{metrics[config.vector_search_index_metric]}_ops
                );
                SET hnsw.ef_search = {ef_search};
            """
            # Enable iterative scan for pgvector v0.8.0 and up.
            pgvector_version = session.execute(
                text("SELECT extversion FROM pg_extension WHERE extname = 'vector'")
            ).scalar_one()
            if pgvector_version and version.parse(pgvector_version) >= version.parse("0.8.0"):
                create_vector_index_sql += f"\nSET hnsw.iterative_scan = {'relaxed_order' if config.reranker else 'strict_order'};"
            session.execute(text(create_vector_index_sql))
            session.commit()
    elif db_backend == "duckdb":
        with Session(engine) as session:
            # Create a keyword search index with FTS if it is missing or out of date. DuckDB does
            # not automatically update the FTS index when the table is modified, so we take the
            # opportunity to update it here on engine creation.
            num_chunks = session.execute(text("SELECT COUNT(*) FROM chunk")).scalar_one()
            try:
                num_indexed_chunks = session.execute(
                    text("SELECT COUNT(*) FROM fts_main_chunk.docs")
                ).scalar_one()
            except ProgrammingError:
                num_indexed_chunks = 0
            if num_indexed_chunks == 0 or num_indexed_chunks != num_chunks:
                session.execute(
                    text("PRAGMA create_fts_index('chunk', 'id', 'body', overwrite = 1);")
                )
            # Create a vector search index with VSS if it doesn't exist.
            session.execute(
                text(f"""
                SET hnsw_ef_search = {ef_search};
                SET hnsw_enable_experimental_persistence = true;
                """)
            )
            vss_index_exists = session.execute(
                text("""
                SELECT COUNT(*) > 0
                FROM duckdb_indexes()
                WHERE schema_name = current_schema()
                AND table_name = 'chunk_embedding'
                AND index_name = 'vector_search_chunk_index'
                """)
            ).scalar_one()
            if not vss_index_exists:
                metrics = {"cosine": "cosine", "dot": "ip", "l2": "l2sq"}
                create_vector_index_sql = f"""
                    CREATE INDEX vector_search_chunk_index
                    ON chunk_embedding
                    USING HNSW (embedding)
                    WITH (metric = '{metrics[config.vector_search_index_metric]}');
                """
                session.execute(text(create_vector_index_sql))
            session.commit()
    return engine
